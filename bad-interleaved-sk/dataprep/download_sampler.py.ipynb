{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOG_FORMAT = \"%(levelname)s %(asctime)s %(message)s\"\n",
    "logging.basicConfig(filename=\"logs/download_sampler.log\"\n",
    "                   , format = LOG_FORMAT\n",
    "                   , level = logging.INFO )\n",
    "logger = logging.getLogger();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data_urls_file = \"config/raw_dataurls.json\"\n",
    "raw_data_urls = json.load(open(raw_data_urls_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_resource= boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate over the array of objects to be procesed\n",
    "1. Download each object from S3 raw data \n",
    "2. Sample the file currently uses head -10000\n",
    "3. Read the CSV file and cleanse the file to remove ControlM characters. Done using pandas\n",
    "4. Write the CSV from Pandas dataframe\n",
    "5. Upload the cleansed object to your personal S3 bucket. \n",
    "6. Remove the following\n",
    "   a. Cleansed sample file\n",
    "   b. Sample file\n",
    "   c. Raw data file downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in range( len(raw_data_urls['rawdatafiles']) ):\n",
    "    try:\n",
    "        logger.info('Start Processing - ' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'])\n",
    "        \n",
    "        logger.info('Started Downloading ' \n",
    "                    + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] \n",
    "                    + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'] \n",
    "                    + ' --> '\n",
    "                    + raw_data_urls['rawdatafiles'][index]['raw_data_download_loc']\n",
    "                   )\n",
    "        s3_resource.Bucket(raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt']\n",
    "                          ).download_file(raw_data_urls['rawdatafiles'][index]['raw_data_s3_object']\n",
    "                                   , raw_data_urls['rawdatafiles'][index]['raw_data_download_loc'])\n",
    "        logger.info('Finished Downloading ' \n",
    "                    + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] \n",
    "                    + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'] \n",
    "                    + ' --> '\n",
    "                    + raw_data_urls['rawdatafiles'][index]['raw_data_download_loc']\n",
    "                   )\n",
    "        samplefile_tocleanse = raw_data_urls['rawdatafiles'][index]['sample_processed_file'] + '.tocleanse'\n",
    "        \n",
    "        # Use os.system head command to sample the file and create a file for cleansing. \n",
    "        cmd = \"head -10000 \" + raw_data_urls['rawdatafiles'][index]['raw_data_download_loc'] + ' > ' + samplefile_tocleanse\n",
    "        os.system (cmd)\n",
    "        logger.info ('Generated sample file to be cleansed ' + samplefile_tocleanse )\n",
    "        \n",
    "        # Handle any cleansing activity using the Python dataframes\n",
    "        df = pd.read_csv(samplefile_tocleanse)\n",
    "        df.to_csv(raw_data_urls['rawdatafiles'][index]['sample_processed_file'], sep=',' , index=False)\n",
    "        \n",
    "        logger.info ('Generated cleansed sample file ' + raw_data_urls['rawdatafiles'][index]['sample_processed_file'] )\n",
    "        \n",
    "        logger.info ('Started uploading ' + raw_data_urls['rawdatafiles'][index]['sample_processed_file'] \n",
    "                     + ' --> ' \n",
    "                     + raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_bkt']\n",
    "                     + '/' +  raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_objet'] )\n",
    "        s3_resource.Bucket(raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_bkt']\n",
    "                          ).upload_file(raw_data_urls['rawdatafiles'][index]['sample_processed_file']\n",
    "                                    , raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_objet'])\n",
    "        logger.info ('Finished uploading ' + raw_data_urls['rawdatafiles'][index]['sample_processed_file'] \n",
    "                     + ' --> ' \n",
    "                     + raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_bkt']\n",
    "                     + '/' +  raw_data_urls['rawdatafiles'][index]['cleansed_sampled_data_s3_objet'] )\n",
    "        \n",
    "        os.remove(raw_data_urls['rawdatafiles'][index]['sample_processed_file'])\n",
    "        logger.info ('Removed sample cleansed file ' + raw_data_urls['rawdatafiles'][index]['sample_processed_file'])\n",
    "        \n",
    "        os.remove(samplefile_tocleanse)\n",
    "        logger.info('Removed sample file to be cleansed - ' + samplefile_tocleanse)\n",
    "        \n",
    "        os.remove(raw_data_urls['rawdatafiles'][index]['raw_data_download_loc'])\n",
    "        logger.info('Removed the downloaded from S3 - ' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'])\n",
    "        \n",
    "        logger.info('Finished Processing - ' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'])\n",
    "    except botocore.exceptions.ClientError as downloadexception:\n",
    "        if downloadexception.response['Error']['Code'] == \"404\":\n",
    "            logger.exception ('The object + '  + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'] + ' does not exist.' )\n",
    "        else:\n",
    "            logger.exception ('Unhandled exception while processing ' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_bkt'] + '/' + raw_data_urls['rawdatafiles'][index]['raw_data_s3_object'])\n",
    "    except Exception as e:\n",
    "            logger.exception('Unhandled generic exception!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
